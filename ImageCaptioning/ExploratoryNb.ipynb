{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5OHaGch1U6m"
   },
   "source": [
    "# HW4: Image Captioning\n",
    "---\n",
    "\n",
    "This is the Notebook that goes with **Homework 4: Image Captioning**! \n",
    "\n",
    "In this notebook, you can visualize the self-attention layer in your TransformerDecoder, and generate captions using both of your models for images in the test dataset. \n",
    "\n",
    "This notebook can be ported to Colab very quickly, so please feel free to try that out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvuKoBRGYAwf"
   },
   "source": [
    "This block of code imports the classes you completed in your assignment, along with additional libraries needed for the visualizations.\n",
    "\n",
    "Feel free to add autoimport queries as needed. This notebook's code will not be auto-ran by the autograder (only the outputs will be looked at during manual grading), so do what you need to here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARiGr47j7T-I"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from model import ImageCaptionModel\n",
    "from decoder import TransformerDecoder, RNNDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment uses the Flickr 8k dataset! Let's go ahead and pull that in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before this, download the dataset and run preprocessing.py as instructed. \n",
    "## This may take like 10 mins, but should only happen once so ok.\n",
    "## https://www.kaggle.com/datasets/adityajn105/flickr8k?resource=download\n",
    "\n",
    "with open('../data/data.p', 'rb') as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# As mentioned in the handout, this assignment has 5 captions per image. This block of code \n",
    "# expands the image_feature lists to have 5 copies of each image to correspond to each of their captions \n",
    "feat_prep = lambda x: np.repeat(np.array(x).reshape(-1, 2048), 5, axis=0)\n",
    "img_prep  = lambda x: np.repeat(x/255., 5, axis=0).astype(np.float64)\n",
    "\n",
    "## Captions; preprocessed sentences with 20 window size\n",
    "train_captions  = np.array(data_dict['train_captions'],dtype=np.int64);            print('train_captions:  ', train_captions.shape)\n",
    "test_captions   = np.array(data_dict['test_captions'],dtype=np.int64);             print('test_captions:   ', test_captions.shape)\n",
    "\n",
    "## 2048-D resnet embeddings of images.\n",
    "train_img_feats = feat_prep(data_dict['train_image_features']);     print('\\ntrain_img_feats: ', train_img_feats.shape)\n",
    "test_img_feats  = feat_prep(data_dict['test_image_features']);      print('test_img_feats:  ', test_img_feats.shape)\n",
    "\n",
    "## Small subset of actual images for visualization purposes. \n",
    "## These are just for the first 100 images of each (clones 5 times)\n",
    "train_images    = img_prep(data_dict['train_images']);              print('\\ntrain_images:    ', train_images.shape)\n",
    "test_images     = img_prep(data_dict['test_images']);               print('test_images:     ', test_images.shape)\n",
    "\n",
    "## Conversion dictionaries to go between word and label index\n",
    "word2idx        = data_dict['word2idx']\n",
    "idx2word        = data_dict['idx2word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the images take up a lot of data, we only kept a sliver of the original images. Feel free to update the preprocessing to retain all of the images if you'd like. Below is a visualization of some of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        print(f'Caption {j+1}:', ' '.join([idx2word[idx] for idx in train_captions[i * 5 + j]]))\n",
    "    plt.imshow(train_images[i * 5])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71_bkKyU0iIq"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "After training our Transformer model, you can visualize the self-attention layer to examine the behavior of your attention heads and see if any patterns emerge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpz85avktKxt"
   },
   "source": [
    "To test out the components of the model interactively, you'll need to deconstruct selections of the model/runner code and get an instance of the model in an interactive context (aka inside the notebook). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feel free to insert auto-reloads as necessary\n",
    "from assignment import parse_args, load_model\n",
    "from decoder import TransformerDecoder, RNNDecoder\n",
    "\n",
    "## TODO: Pull your model into the notebook. This is heavily based off of assignment.py, \n",
    "##   and feel free to reuse as much as you want. Your final project will probably \n",
    "##   involve a lot of this investigative reverse-engineering based on what repos \n",
    "##   you have to stumble upon.\n",
    "##   You're not in a notebook scenario, so use get_default_arguments and feel free to update it...\n",
    "\n",
    "args = parse_args('--type rnn --task both --data ../data/data.p'.split())\n",
    "\n",
    "args.chkpt_path = '../transform_model'\n",
    "tra_imcap = load_model(args)\n",
    "\n",
    "args.chkpt_path = '../rnn_model'\n",
    "rnn_imcap = load_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_imcap.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_imcap.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, we need to be able to actually access the attention matrix that gets generated by out model. So that we can visualize it, right? Unfortunately for us, some convenience methods that allow you to make arbitrary model slices (i.e. the Functional API) are forfeit since our model is a subclass (in contrast to a sequential or functional). \n",
    "\n",
    "However, we can still dig into the model and force out way to computing the components we want. Our weights have been saved, after all..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yChcaClyump_"
   },
   "source": [
    "The following block of code visualizes the decoder self-attention for a random images in the test dataset. \n",
    "\n",
    "\n",
    "Move your mouse over the words in the left hand column, and see how much attention your decoder self-attention layer pays to each word in the sentance as it encodes each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4y4cKlpWvwUf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from vis_utils import plot_decoder_text_attention\n",
    "import numpy as np\n",
    "\n",
    "index = np.random.choice(np.array(list(range(0,500,5))))\n",
    "\n",
    "caption    = test_captions[index][:-1]\n",
    "image_feat = test_img_feats[index]\n",
    "image      = test_images[index]\n",
    "\n",
    "print(\"Image number:\", index)\n",
    "\n",
    "def get_attention(tra_imcap, image_feat, caption):\n",
    "    ## TODO If you're implementing multi-headed attension, will \n",
    "    ## need to update this method to show off your multiple heads and \n",
    "    ## the overall aggregate. \n",
    "\n",
    "    ## Into impac decoder\n",
    "    encoded_images = tra_imcap.decoder.image_embedding(tf.expand_dims(image_feat, 1))\n",
    "    # captions = tra_imcap.decoder.embedding(caption)\n",
    "    captions = tra_imcap.decoder.encoding(caption)\n",
    "    ## Into imcap TransformerBlock; get self-attention\n",
    "    AttentionHead = tra_imcap.decoder.decoder.self_atten\n",
    "    K = tf.tensordot(captions, AttentionHead.K, 1)\n",
    "    V = tf.tensordot(captions, AttentionHead.V, 1)\n",
    "    self_atten = AttentionHead.attn_mtx((K, V))\n",
    "    ## Into imcap TransformerBlock; get context self-attention\n",
    "    AttentionHead = tra_imcap.decoder.decoder.self_context_atten\n",
    "    K = tf.tensordot(captions, AttentionHead.K, 1)\n",
    "    V = tf.tensordot(captions, AttentionHead.V, 1)\n",
    "    self_context_atten = AttentionHead.attn_mtx((K, V))\n",
    "    return self_atten, self_context_atten\n",
    "\n",
    "\n",
    "def vis_attention(atten_mtx, image_features, caption, idx2word):\n",
    "    caption_words = [idx2word[idx] for idx in caption]\n",
    "    end_sentance_index = caption_words.index('<end>') if '<end>' in caption_words else 20\n",
    "    caption_words = caption_words[:end_sentance_index]\n",
    "    AttentionMatrix = atten_mtx[:, :end_sentance_index, :end_sentance_index]\n",
    "    AttentionMatrix = tf.reshape(AttentionMatrix, (1, 1, 1, end_sentance_index, end_sentance_index))\n",
    "    plot_decoder_text_attention(attention=AttentionMatrix, tokens=caption_words)\n",
    "\n",
    "self_atten, self_context_atten = get_attention(\n",
    "    tra_imcap, tf.expand_dims(image_feat, 0), tf.expand_dims(caption, 0)\n",
    ")\n",
    "\n",
    "print(\"self_atten\")\n",
    "vis_attention(self_atten, image_feat, caption, idx2word)\n",
    "\n",
    "print(\"self_context_atten\")\n",
    "vis_attention(self_context_atten, image_feat, caption, idx2word)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTTYAclU7ALb"
   },
   "source": [
    "### Caption Generation\n",
    "Now that you have trained both of your models, it's time to use them to generate original captions for images in the testing set. First, the model is given the <start\\> token and asked to generate probabilites for the next word in the sequence. The next token is chosen by sampling from that probability. This process repeats until the model generates the <end\\> token, or the maximum sequence length is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zar8W0Zn7Lnr"
   },
   "source": [
    " \n",
    "\n",
    "\n",
    "\n",
    "There is still one piece of this equation missing. The tokens are sampled from the probabilities your models generate, but your models were required to output logits, not probabilities. This is becasue this assignment, like many NLP models, uses temperature as a parameter in text generation. If the models sampled from  probabilies calculated by simply applying softmax to the logits, then the probability of the most likely word will usually be very high and the models will usually genrate the same, most probable caption every time. We use the temperature as a parameter to even out the probabilites so the model produces more 'creative' captions. This is done by dividing the logits by the temperature parameter before applying softmax. Higher temprature values will give a more creative captiong, while temprature values closer to 0 will be more greedy. Check out [this](https://lukesalamone.github.io/posts/what-is-temperature/) article for a demonstration and further explaination of temprature in NLP models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVNzx9-qP_Th"
   },
   "source": [
    "The following blocks of code will generate a caption for the image currently selected for the attention visualization above. Try playing around with different temperature values and see how it changes the captions your models generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_DpuFiYMOIa"
   },
   "outputs": [],
   "source": [
    "def gen_caption_temperature(model, image_embedding, wordToIds, padID, temp, window_length):\n",
    "    \"\"\"\n",
    "    Function used to generate a caption using an ImageCaptionModel given\n",
    "    an image embedding. \n",
    "    \"\"\"\n",
    "    idsToWords = {id: word for word, id in wordToIds.items()}\n",
    "    unk_token = wordToIds['<unk>']\n",
    "    caption_so_far = [wordToIds['<start>']]\n",
    "    while len(caption_so_far) < window_length and caption_so_far[-1] != wordToIds['<end>']:\n",
    "        caption_input = np.array([caption_so_far + ((window_length - len(caption_so_far)) * [padID])])\n",
    "        logits = model(np.expand_dims(image_embedding, 0), caption_input)\n",
    "        logits = logits[0][len(caption_so_far) - 1]\n",
    "        probs = tf.nn.softmax(logits / temp).numpy()\n",
    "        next_token = unk_token\n",
    "        attempts = 0\n",
    "        while next_token == unk_token and attempts < 5:\n",
    "            next_token = np.random.choice(len(probs), p=probs)\n",
    "            attempts += 1\n",
    "        caption_so_far.append(next_token)\n",
    "    return ' '.join([idsToWords[x] for x in caption_so_far][1:-1])\n",
    "\n",
    "temperature = .05\n",
    "gen_caption_temperature(tra_imcap, image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NNogJ5TL_-v"
   },
   "outputs": [],
   "source": [
    "temperature = 0.2\n",
    "gen_caption_temperature(tra_imcap, image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You may want to try a different image. Sometimes you get really unlucky with random selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sentences for Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.05\n",
    "indices = np.random.choice(np.array(list(range(0, 500, 5))), 10, replace=False)\n",
    "for i in indices:\n",
    "    curr_image_feat = train_img_feats[i]\n",
    "    curr_image      = train_images[i]\n",
    "    for j in range(5):  ## Display all of the captions trained on\n",
    "        words = [idx2word[x] for x in train_captions[i+j][:-1] if idx2word[x] not in ('<pad>', '<start>', '<end>')]\n",
    "        print(f'C{j+1}:', ' '.join(words))\n",
    "    print('RNN:', gen_caption_temperature(rnn_imcap, curr_image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size))\n",
    "    print('TRA:', gen_caption_temperature(tra_imcap, curr_image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size))\n",
    "    plt.imshow(curr_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out on things in testing set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.05\n",
    "indices = np.random.choice(np.array(list(range(0, 500, 5))), 10, replace=False)\n",
    "for i in indices:\n",
    "    curr_image_feat = test_img_feats[i]\n",
    "    curr_image      = test_images[i]\n",
    "    for j in range(5):  ## Display all of the captions trained on\n",
    "        words = [idx2word[x] for x in test_captions[i+j][:-1] if idx2word[x] not in ('<pad>', '<start>', '<end>')]\n",
    "        print(f'C{j+1}:', ' '.join(words))\n",
    "    print('RNN:', gen_caption_temperature(rnn_imcap, curr_image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size))\n",
    "    print('TRA:', gen_caption_temperature(tra_imcap, curr_image_feat, word2idx, word2idx['<pad>'], temperature, args.window_size))\n",
    "    plt.imshow(curr_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpwoQazfkfv1"
   },
   "source": [
    "# Conclusion!\n",
    "Congrats! You have finished this assignment! Below, put down your favorite captions that your RNN and Transformer models both generated!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0n-1aiQFliGn"
   },
   "outputs": [],
   "source": [
    "#Todo: fill in the ? and display the vis images with the generated caption below it\n",
    "\n",
    "rnn_image_index = ?\n",
    "rnn_caption = gen_caption_temperature(rnn_imcap, test_img_feats[rnn_image_index], word2idx, word2idx['<pad>'], temperature, args.window_size)\n",
    "\n",
    "tra_image_index = ?\n",
    "tra_caption = gen_caption_temperature(tra_imcap, test_img_feats[tra_image_index], word2idx, word2idx['<pad>'], temperature, args.window_size)\n",
    "\n",
    "print(rnn_caption)\n",
    "plt.imshow(test_images[rnn_image_index])\n",
    "plt.show()\n",
    "\n",
    "print(tra_caption)\n",
    "plt.imshow(test_images[tra_image_index])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "im_cap_notebook.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "895b6b526044b58cdb0796603b6137eb4df401700b6bb2bfb9582034a97581c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
